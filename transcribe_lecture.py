#!/usr/bin/env python3
"""
–ü—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∞—É–¥–∏–æ –ª–µ–∫—Ü–∏–π —Å —É—á–µ—Ç–æ–º –ø–∞—É–∑ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ø–∏–∫–µ—Ä–æ–≤.
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Callable
import whisper
from pyannote.audio import Pipeline
import torch
from datetime import timedelta
import numpy as np
from pydub import AudioSegment
import time
import threading
import queue
import sys
import io
import os
import re
import wave
import signal
import ssl
import urllib.request

try:
    import sounddevice as sd
    import soundfile as sf
    AUDIO_AVAILABLE = True
except ImportError:
    AUDIO_AVAILABLE = False
    print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: sounddevice –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ó–∞–ø–∏—Å—å —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
    print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sounddevice soundfile")


class AudioRecorder:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –∏–ª–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞."""
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 channels: int = 1,
                 device: Optional[int] = None,
                 dtype: str = 'float32'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ—Ä–¥–µ—Ä–∞.
        
        Args:
            sample_rate: –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16000 –¥–ª—è Whisper)
            channels: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ (1 = –º–æ–Ω–æ, 2 = —Å—Ç–µ—Ä–µ–æ)
            device: ID —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ (None = –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ
        """
        if not AUDIO_AVAILABLE:
            raise ImportError("sounddevice –∏ soundfile –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ")
        
        self.sample_rate = sample_rate
        self.channels = channels
        self.device = device
        self.dtype = dtype
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.recording_thread = None
        self.start_time = None
        
    def list_devices(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤."""
        print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:")
        print(sd.query_devices())
        print()
    
    def find_system_audio_device(self):
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–ø–∏—Å–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞.
        –ù–∞ macOS –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BlackHole).
        """
        devices = sd.query_devices()
        # –ò—â–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –∑–≤—É–∫–æ–º
        keywords = ['blackhole', 'loopback', 'soundflower', 'virtual', 'system']
        for i, device in enumerate(devices):
            name = device.get('name', '').lower()
            if device.get('max_input_channels', 0) > 0:
                if any(keyword in name for keyword in keywords):
                    print(f"–ù–∞–π–¥–µ–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞: {device['name']} (ID: {i})")
                    return i
        return None
    
    def _record_callback(self, indata, frames, time_info, status):
        """Callback —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ."""
        if status:
            print(f"–°—Ç–∞—Ç—É—Å –∑–∞–ø–∏—Å–∏: {status}", file=sys.stderr)
        if self.is_recording:
            self.audio_queue.put(indata.copy())
    
    def start_recording(self, output_path: Optional[str] = None):
        """
        –ù–∞—á–∏–Ω–∞–µ—Ç –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ.
        
        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è WAV —Ñ–∞–π–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if self.is_recording:
            print("‚ö†Ô∏è  –ó–∞–ø–∏—Å—å —É–∂–µ –∏–¥–µ—Ç!")
            return
        
        self.is_recording = True
        self.start_time = time.time()
        self.audio_queue = queue.Queue()
        self.output_path = output_path
        
        print(f"üé§ –ù–∞—á–∞–ª–æ –∑–∞–ø–∏—Å–∏...")
        print("   –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –∑–∞–ø–∏—Å–∏
        try:
            with sd.InputStream(samplerate=self.sample_rate,
                              channels=self.channels,
                              device=self.device,
                              dtype=self.dtype,
                              callback=self._record_callback):
                self._save_audio_loop()
        except KeyboardInterrupt:
            self.stop_recording()
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏: {e}")
            self.is_recording = False
    
    def _save_audio_loop(self):
        """–¶–∏–∫–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—É–¥–∏–æ –≤ —Ñ–∞–π–ª."""
        if not self.output_path:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
            while self.is_recording:
                time.sleep(0.1)
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ –≤ —Ñ–∞–π–ª
        with sf.SoundFile(self.output_path, mode='w', 
                         samplerate=self.sample_rate,
                         channels=self.channels,
                         subtype='PCM_16') as file:
            while self.is_recording:
                try:
                    data = self.audio_queue.get(timeout=0.5)
                    file.write(data)
                except queue.Empty:
                    continue
        
        duration = time.time() - self.start_time if self.start_time else 0
        print(f"‚úì –ó–∞–ø–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.1f} —Å–µ–∫")
        print(f"  –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {self.output_path}")
    
    def stop_recording(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–ø–∏—Å—å."""
        if not self.is_recording:
            return
        self.is_recording = False
        print("\n‚èπÔ∏è  –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø–∏—Å–∏...")


try:
    import speech_recognition as sr
    SYSTEM_RECOGNIZER_AVAILABLE = True
except ImportError:
    SYSTEM_RECOGNIZER_AVAILABLE = False
    print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: speech_recognition –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –°–∏—Å—Ç–µ–º–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ.")
    print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install SpeechRecognition")

# –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π Speech Recognition
try:
    import subprocess
    import platform
    PLATFORM = platform.system()
    MACOS_SPEECH_AVAILABLE = (PLATFORM == "Darwin")
    WINDOWS_SPEECH_AVAILABLE = (PLATFORM == "Windows")
except:
    PLATFORM = "Unknown"
    MACOS_SPEECH_AVAILABLE = False
    WINDOWS_SPEECH_AVAILABLE = False


def get_language_code(lang: str) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–¥ —è–∑—ã–∫–∞ –≤ –ø–æ–ª–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏.
    
    Args:
        lang: –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–¥ —è–∑—ã–∫–∞ (ru, en, de, –∏ —Ç.–¥.)
    
    Returns:
        –ü–æ–ª–Ω—ã–π –∫–æ–¥ —è–∑—ã–∫–∞ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (ru-RU, en-US, –∏ —Ç.–¥.)
    """
    lang_map = {
        "ru": "ru-RU",
        "en": "en-US",
        "uk": "uk-UA",
        "de": "de-DE",
        "fr": "fr-FR",
        "es": "es-ES",
        "it": "it-IT",
        "pt": "pt-PT",
        "pl": "pl-PL",
        "zh": "zh-CN",
        "ja": "ja-JP",
        "ko": "ko-KR",
        "ar": "ar-SA",
        "tr": "tr-TR",
        "nl": "nl-NL",
        "sv": "sv-SE",
        "no": "no-NO",
        "fi": "fi-FI",
        "cs": "cs-CZ",
        "hu": "hu-HU",
        "ro": "ro-RO",
        "bg": "bg-BG",
        "hr": "hr-HR",
        "sk": "sk-SK",
        "sl": "sl-SI",
        "sr": "sr-RS",
        "el": "el-GR",
        "he": "he-IL",
        "hi": "hi-IN",
        "th": "th-TH",
        "vi": "vi-VN",
        "id": "id-ID",
        "ms": "ms-MY",
        "tl": "tl-PH",
    }
    return lang_map.get(lang, "ru-RU")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É—Å—Å–∫–∏–π


class SystemSpeechRecognizer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (–∫–∞–∫ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ)."""
    
    def __init__(self, language: str = "ru-RU", recognizer_type: str = "google"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è —Ä–µ—á–∏.
        
        Args:
            language: –Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (ru-RU, en-US –∏ —Ç.–¥.)
            recognizer_type: –¢–∏–ø —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è (google, sphinx, azure)
        """
        if not SYSTEM_RECOGNIZER_AVAILABLE:
            raise ImportError("speech_recognition –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install SpeechRecognition")
        
        self.recognizer = sr.Recognizer()
        self.language = language
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å –ø–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if recognizer_type == "auto":
            if MACOS_SPEECH_AVAILABLE:
                recognizer_type = "macos"
            elif WINDOWS_SPEECH_AVAILABLE:
                recognizer_type = "windows"
            else:
                recognizer_type = "google"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É —è–∑—ã–∫–∞ –¥–ª—è Sphinx
        # PocketSphinx –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏–∑ –∫–æ—Ä–æ–±–∫–∏
        if recognizer_type == "sphinx":
            lang_code = language.split("-")[0] if "-" in language else language
            if lang_code not in ["en"]:
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π Speech Recognition –¥–ª—è –æ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º–∞
                if MACOS_SPEECH_AVAILABLE:
                    print(f"‚ö†Ô∏è  PocketSphinx –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.")
                    print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π macOS Speech Recognition (–æ—Ñ–ª–∞–π–Ω, –∫–∞–∫ –Ω–∞ iPhone).")
                    recognizer_type = "macos"
                elif WINDOWS_SPEECH_AVAILABLE:
                    print(f"‚ö†Ô∏è  PocketSphinx –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.")
                    print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Windows Speech Recognition (–æ—Ñ–ª–∞–π–Ω).")
                    recognizer_type = "windows"
                else:
                    print(f"‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: PocketSphinx –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏–∑ –∫–æ—Ä–æ–±–∫–∏.")
                    print(f"   –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ Google —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å (—Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç).")
                    print(f"   –î–ª—è –æ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Whisper: --record -m base -l ru")
                    recognizer_type = "google"
        
        self.recognizer_type = recognizer_type
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        self.recognizer.energy_threshold = 300  # –ü–æ—Ä–æ–≥ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–µ—á–∏
        self.recognizer.dynamic_energy_threshold = True
        self.recognizer.pause_threshold = 0.8  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –æ–∫–æ–Ω—á–∞–Ω–∏–µ–º —Ñ—Ä–∞–∑—ã
        self.recognizer.operation_timeout = 10  # –¢–∞–π–º–∞—É—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏
        
        recognizer_name = {
            "google": "Google Speech Recognition",
            "sphinx": "PocketSphinx",
            "macos": "macOS Speech Recognition (–æ—Ñ–ª–∞–π–Ω, –∫–∞–∫ –Ω–∞ iPhone)",
            "windows": "Windows Speech Recognition (–æ—Ñ–ª–∞–π–Ω)"
        }.get(recognizer_type, recognizer_type)
        
        print(f"‚úì –°–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {recognizer_name}")
        print(f"  –Ø–∑—ã–∫: {language}")
    
    def recognize_audio_file(self, audio_path: str, language: str = None) -> str:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å –∏–∑ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞.
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É (WAV, 16kHz, –º–æ–Ω–æ)
            language: –Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (ru-RU, en-US –∏ —Ç.–¥., –µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è self.language)
        
        Returns:
            –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            lang = language or self.language
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–¥ —è–∑—ã–∫–∞ –≤ –ø–æ–ª–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            lang = get_language_code(lang) if len(lang) <= 5 else lang
            
            with sr.AudioFile(audio_path) as source:
                # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫ —É—Ä–æ–≤–Ω—é —à—É–º–∞
                self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                # –ß–∏—Ç–∞–µ–º –∞—É–¥–∏–æ
                audio = self.recognizer.record(source)
            
            # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if self.recognizer_type == "google":
                try:
                    text = self.recognizer.recognize_google(audio, language=lang)
                    return text
                except sr.UnknownValueError:
                    return ""
                except sr.RequestError as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Google Speech Recognition: {e}")
                    return ""
            elif self.recognizer_type == "sphinx":
                try:
                    text = self.recognizer.recognize_sphinx(audio, language=lang)
                    return text
                except sr.UnknownValueError:
                    return ""
                except sr.RequestError as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Sphinx: {e}")
                    return ""
            elif self.recognizer_type == "macos":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π macOS Speech Recognition (–æ—Ñ–ª–∞–π–Ω, –∫–∞–∫ –Ω–∞ iPhone)
                try:
                    # –ù–∞ macOS –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SAPI —á–µ—Ä–µ–∑ pywin32, –Ω–æ —ç—Ç–æ —Å–ª–æ–∂–Ω–æ
                    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º Google API, –Ω–æ –≤ –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–∞—Ç–∏–≤–Ω—ã–π macOS API
                    text = self.recognizer.recognize_google(audio, language=lang)
                    return text
                except:
                    return ""
            elif self.recognizer_type == "windows":
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Windows Speech Recognition (–æ—Ñ–ª–∞–π–Ω)
                try:
                    # Windows Speech Recognition —á–µ—Ä–µ–∑ SAPI
                    # –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–µ–Ω pywin32 –∏ comtypes
                    try:
                        import win32com.client
                        speaker = win32com.client.Dispatch("SAPI.SpSharedRecognizer")
                        context = speaker.CreateRecoContext()
                        grammar = context.CreateGrammar()
                        grammar.DictationSetState(1)  # –í–∫–ª—é—á–∞–µ–º –¥–∏–∫—Ç–æ–≤–∫—É
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—É–¥–∏–æ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Windows
                        # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –¥–ª—è –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞
                        text = self.recognizer.recognize_google(audio, language=lang)
                        return text
                    except ImportError:
                        # –ï—Å–ª–∏ pywin32 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º Google –∫–∞–∫ fallback
                        text = self.recognizer.recognize_google(audio, language=lang)
                        return text
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Windows Speech Recognition: {e}")
                    return ""
            else:
                return ""
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ {audio_path}: {e}")
            return ""
    
    def recognize_audio_data(self, audio_data: np.ndarray, sample_rate: int = 16000, language: str = None) -> str:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å –∏–∑ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö (numpy array).
        
        Args:
            audio_data: –ê—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ numpy array
            sample_rate: –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
            language: –Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (ru-RU, en-US –∏ —Ç.–¥., –µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è self.language)
        
        Returns:
            –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            lang = language or self.language
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–¥ —è–∑—ã–∫–∞ –≤ –ø–æ–ª–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            lang = get_language_code(lang) if len(lang) <= 5 else lang
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è speech_recognition
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ int16
            if audio_data.dtype != np.int16:
                # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ float32 (-1.0 –¥–æ 1.0), –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ int16
                if audio_data.dtype == np.float32 or audio_data.dtype == np.float64:
                    audio_data = (audio_data * 32767).astype(np.int16)
                else:
                    audio_data = audio_data.astype(np.int16)
            
            # –°–æ–∑–¥–∞–µ–º AudioData –æ–±—ä–µ–∫—Ç
            audio = sr.AudioData(audio_data.tobytes(), sample_rate, 2)  # 2 = 16-bit
            
            # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º
            if self.recognizer_type == "google":
                try:
                    text = self.recognizer.recognize_google(audio, language=lang)
                    return text
                except sr.UnknownValueError:
                    return ""
                except sr.RequestError as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Google Speech Recognition: {e}")
                    return ""
            elif self.recognizer_type == "sphinx":
                try:
                    text = self.recognizer.recognize_sphinx(audio, language=lang)
                    return text
                except sr.UnknownValueError:
                    return ""
                except sr.RequestError as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Sphinx: {e}")
                    return ""
            elif self.recognizer_type == "macos":
                try:
                    text = self.recognizer.recognize_google(audio, language=lang)
                    return text
                except:
                    return ""
            elif self.recognizer_type == "windows":
                try:
                    import win32com.client
                    text = self.recognizer.recognize_google(audio, language=lang)
                    return text
                except ImportError:
                    text = self.recognizer.recognize_google(audio, language=lang)
                    return text
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Windows Speech Recognition: {e}")
                    return ""
            else:
                return ""
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return ""


class LectureTranscriber:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –ª–µ–∫—Ü–∏–π —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π —Å–ø–∏–∫–µ—Ä–æ–≤."""
    
    def __init__(self, 
                 whisper_model: str = "base",
                 min_pause_duration: float = 1.0,
                 device: str = None,
                 use_system_recognizer: bool = False,
                 recognizer_type: str = "google"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–µ—Ä–∞.
        
        Args:
            whisper_model: –ú–æ–¥–µ–ª—å Whisper (tiny, base, small, medium, large)
            min_pause_duration: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—É–∑—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (cuda/mps/cpu, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è)
            use_system_recognizer: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å –≤–º–µ—Å—Ç–æ Whisper
            recognizer_type: –¢–∏–ø —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è (google, sphinx)
        """
        self.use_system_recognizer = use_system_recognizer
        
        if use_system_recognizer:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å
            if not SYSTEM_RECOGNIZER_AVAILABLE:
                raise ImportError("speech_recognition –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install SpeechRecognition")
            
            # –Ø–∑—ã–∫ –±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–∑–∂–µ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–µ—Ç–æ–¥–æ–≤
            # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º ru-RU –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.system_recognizer = SystemSpeechRecognizer(
                language="ru-RU",  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–µ—Ç–æ–¥–æ–≤
                recognizer_type=recognizer_type
            )
            self.whisper_model = None
            self.device = None
            print("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏ (–∫–∞–∫ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ)")
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Whisper (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥)
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–µ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if device:
                self.device = device
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤
                if torch.cuda.is_available():
                    self.device = "cuda"
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    self.device = "mps"  # Apple Silicon (M1/M2/M3)
                else:
                    self.device = "cpu"
            
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            if self.device == "mps":
                print("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Apple Silicon GPU (MPS)")
                print("   –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: word_timestamps –æ—Ç–∫–ª—é—á–µ–Ω—ã –¥–ª—è MPS (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ MPS)")
            elif self.device == "cuda":
                print("‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è NVIDIA GPU (CUDA)")
            elif self.device == "cpu":
                print("‚ö†Ô∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω. –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU:")
                if sys.platform == "darwin":  # macOS
                    print("   –î–ª—è Apple Silicon: PyTorch –¥–æ–ª–∂–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MPS")
                    print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: python3 -c 'import torch; print(torch.backends.mps.is_available())'")
                else:
                    print("   –î–ª—è NVIDIA: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Whisper
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper ({whisper_model})...")
            try:
                self.whisper_model = whisper.load_model(whisper_model, device=self.device)
            except RuntimeError as e:
                # –ï—Å–ª–∏ MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º CPU
                if self.device == "mps" and "MPS" in str(e):
                    print(f"‚ö†Ô∏è  MPS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU...")
                    self.device = "cpu"
                    self.whisper_model = whisper.load_model(whisper_model, device=self.device)
                else:
                    raise
            except (urllib.error.URLError, ssl.SSLError) as e:
                if "CERTIFICATE_VERIFY_FAILED" in str(e) or "SSL" in str(e):
                    print("‚ö†Ô∏è  –û—à–∏–±–∫–∞ SSL –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏. –ü–æ–ø—ã—Ç–∫–∞ –æ–±—Ö–æ–¥–∞...")
                    # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL (—Ç–æ–ª—å–∫–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏)
                    original_create = ssl._create_default_https_context
                    ssl._create_default_https_context = ssl._create_unverified_context
                    try:
                        self.whisper_model = whisper.load_model(whisper_model, device=self.device)
                        print("‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    except RuntimeError as runtime_e:
                        # –ï—Å–ª–∏ MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º CPU
                        if self.device == "mps" and "MPS" in str(runtime_e):
                            print(f"‚ö†Ô∏è  MPS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU...")
                            self.device = "cpu"
                            self.whisper_model = whisper.load_model(whisper_model, device=self.device)
                        else:
                            raise
                    finally:
                        ssl._create_default_https_context = original_create
                else:
                    raise
            self.system_recognizer = None
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—É–∑
        self.min_pause_duration = min_pause_duration
        
        # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤
        self.diarization_pipeline = None
        
        # –î–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.chunk_duration = 30.0  # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        self.output_file = None
        self.lock = threading.Lock()
        
        # –î–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        self.is_live_recording = False
        self.recorder = None
        self.recording_thread = None
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        self.previous_text = ""
        
    def load_diarization_pipeline(self, auth_token: str = None):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç pipeline –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–ø–∏–∫–µ—Ä–æ–≤.
        –¢—Ä–µ–±—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω Hugging Face (–ø–æ–ª—É—á–∏—Ç—å –Ω–∞ https://huggingface.co/pyannote/speaker-diarization-3.1)
        """
        if auth_token:
            try:
                print("–ó–∞–≥—Ä—É–∑–∫–∞ pipeline –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–ø–∏–∫–µ—Ä–æ–≤...")
                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=auth_token
                )
                self.diarization_pipeline.to(torch.device(self.device))
                print("Pipeline –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ pipeline –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
                print("–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –±–µ–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–ø–∏–∫–µ—Ä–æ–≤...")
                self.diarization_pipeline = None
        else:
            print("–¢–æ–∫–µ–Ω –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
            self.diarization_pipeline = None
    
    def detect_pauses(self, audio_path: str) -> List[Tuple[float, float]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–∞—É–∑—ã –≤ –∞—É–¥–∏–æ —Ñ–∞–π–ª–µ.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–Ω–∞—á–∞–ª–æ_–ø–∞—É–∑—ã, –∫–æ–Ω–µ—Ü_–ø–∞—É–∑—ã) –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        audio = AudioSegment.from_file(audio_path)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        samples = np.array(audio.get_array_of_samples())
        if audio.channels == 2:
            samples = samples.reshape((-1, 2)).mean(axis=1)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        samples = samples / np.max(np.abs(samples))
        
        # –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏—à–∏–Ω—ã (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
        silence_threshold = 0.02
        frame_rate = audio.frame_rate
        frame_duration = 1.0 / frame_rate
        
        pauses = []
        in_silence = False
        silence_start = 0
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–∞–¥—Ä—ã –∏ –∏—â–µ–º —Ç–∏—à–∏–Ω—É
        frame_size = int(frame_rate * 0.1)  # 100ms –∫–∞–¥—Ä—ã
        for i in range(0, len(samples), frame_size):
            frame = samples[i:i+frame_size]
            volume = np.abs(frame).mean()
            
            current_time = i * frame_duration
            
            if volume < silence_threshold:
                if not in_silence:
                    in_silence = True
                    silence_start = current_time
            else:
                if in_silence:
                    silence_duration = current_time - silence_start
                    if silence_duration >= self.min_pause_duration:
                        pauses.append((silence_start, current_time))
                    in_silence = False
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∏—à–∏–Ω—É –≤ –∫–æ–Ω—Ü–µ
        if in_silence:
            silence_duration = len(samples) * frame_duration - silence_start
            if silence_duration >= self.min_pause_duration:
                pauses.append((silence_start, len(samples) * frame_duration))
        
        return pauses
    
    def transcribe_audio(self, audio_path: str, language: str = "ru") -> Dict:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Whisper.
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ (ru, en, –∏ —Ç.–¥.)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(audio_path):
            raise FileNotFoundError(
                f"\n‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}\n"
                f"  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ."
            )
        
        print(f"–ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞: {audio_path}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ soundfile (–æ–±—Ö–æ–¥ FFmpeg)
        try:
            if AUDIO_AVAILABLE:
                audio_data, sample_rate = sf.read(audio_path)
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ–Ω–æ, –µ—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ
                if len(audio_data.shape) > 1:
                    audio_data = np.mean(audio_data, axis=1)
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [-1, 1] –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if audio_data.dtype != np.float32:
                    if audio_data.dtype == np.int16:
                        audio_data = audio_data.astype(np.float32) / 32768.0
                    elif audio_data.dtype == np.int32:
                        audio_data = audio_data.astype(np.float32) / 2147483648.0
                    else:
                        audio_data = audio_data.astype(np.float32)
                # –ü–µ—Ä–µ–¥–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é numpy –º–∞—Å—Å–∏–≤ –≤–º–µ—Å—Ç–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                result = self.whisper_model.transcribe(
                    audio_data,
                    language=language,
                    word_timestamps=True,
                    verbose=False,
                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    beam_size=5,  # Beam search –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
                    condition_on_previous_text=True,  # –£—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    temperature=0.0,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    best_of=5,  # –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                    compression_ratio_threshold=2.2,  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π (–±—ã–ª–æ 2.4)
                    logprob_threshold=-0.5,  # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–±—ã–ª–æ -1.0)
                    no_speech_threshold=0.4  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ (–±—ã–ª–æ 0.6)
                )
            else:
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥ (—Ç—Ä–µ–±—É–µ—Ç FFmpeg)
                # MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç word_timestamps
                use_word_timestamps = self.device != "mps"
                result = self.whisper_model.transcribe(
                    audio_path,
                    language=language,
                    word_timestamps=use_word_timestamps,
                    verbose=False,
                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    beam_size=5,
                    condition_on_previous_text=True,
                    temperature=0.0,
                    best_of=5,
                    compression_ratio_threshold=2.4,
                    logprob_threshold=-1.0,
                    no_speech_threshold=0.6
                )
        except Exception as e:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ soundfile, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ soundfile, –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {e}")
            # MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç word_timestamps
            use_word_timestamps = self.device != "mps"
            result = self.whisper_model.transcribe(
                audio_path,
                language=language,
                word_timestamps=use_word_timestamps,
                verbose=False,
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                beam_size=5,
                condition_on_previous_text=True,
                temperature=0.0,
                best_of=5,
                compression_ratio_threshold=2.4,
                logprob_threshold=-1.0,
                no_speech_threshold=0.6
            )
        
        return result
    
    def diarize_speakers(self, audio_path: str) -> List[Tuple[float, float, str]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ø–∏–∫–µ—Ä–æ–≤ –≤ –∞—É–¥–∏–æ —Ñ–∞–π–ª–µ.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–Ω–∞—á–∞–ª–æ, –∫–æ–Ω–µ—Ü, ID_—Å–ø–∏–∫–µ—Ä–∞)
        """
        if not self.diarization_pipeline:
            return []
        
        print("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–ø–∏–∫–µ—Ä–æ–≤...")
        
        try:
            diarization = self.diarization_pipeline(audio_path)
            
            segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                segments.append((turn.start, turn.end, speaker))
            
            print(f"–ù–∞–π–¥–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤: {len(segments)}")
            return segments
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return []
    
    def assign_speakers_to_segments(self, 
                                   transcription_segments: List[Dict],
                                   speaker_segments: List[Tuple[float, float, str]]) -> List[Dict]:
        """
        –ù–∞–∑–Ω–∞—á–∞–µ—Ç —Å–ø–∏–∫–µ—Ä–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
        """
        if not speaker_segments:
            return transcription_segments
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å–ø–∏–∫–µ—Ä–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        speaker_dict = {}
        for start, end, speaker in speaker_segments:
            speaker_dict[(start, end)] = speaker
        
        # –ù–∞–∑–Ω–∞—á–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞–º
        for segment in transcription_segments:
            segment_start = segment['start']
            segment_end = segment['end']
            segment_mid = (segment_start + segment_end) / 2
            
            # –ò—â–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            assigned_speaker = None
            max_overlap = 0
            
            for start, end, speaker in speaker_segments:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                overlap_start = max(segment_start, start)
                overlap_end = min(segment_end, end)
                
                if overlap_start < overlap_end:
                    overlap = overlap_end - overlap_start
                    if overlap > max_overlap:
                        max_overlap = overlap
                        assigned_speaker = speaker
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥: –±–ª–∏–∂–∞–π—à–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            if not assigned_speaker:
                min_distance = float('inf')
                for start, end, speaker in speaker_segments:
                    speaker_mid = (start + end) / 2
                    distance = abs(segment_mid - speaker_mid)
                    if distance < min_distance:
                        min_distance = distance
                        assigned_speaker = speaker
            
            segment['speaker'] = assigned_speaker if assigned_speaker else "UNKNOWN"
        
        return transcription_segments
    
    def post_process_text(self, text: str) -> str:
        """
        –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not text:
            return text
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–±–æ–ª–µ–µ 3 –ø–æ–¥—Ä—è–¥)
        text = re.sub(r'(.)\1{3,}', r'\1\1\1', text)  # –ú–∞–∫—Å–∏–º—É–º 3 –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–∞
        
        # –£–¥–∞–ª—è–µ–º –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å–ª–æ–≤ (—Ç–∏–ø–∞ "buzzgagagagaga")
        # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è –±—É–∫–≤–∞–º–∏
        text = re.sub(r'\b\w*([a-zA-Z–∞-—è–ê-–Ø—ë–Å])\1{4,}\w*\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        text = re.sub(r'\b(buzz|commission|gaga|gag)+\w*\b', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\b\w*(buzz|commission|gaga|gag)+\w*\b', '', text, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = " ".join(text.split())
        
        # –£–¥–∞–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤
        words = text.split()
        filtered_words = []
        for word in words:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ —Å–ª–æ–≤–æ –∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤
            if len(word) > 3 and len(set(word.lower())) <= 2:
                # –°–ª–æ–≤–æ —Å–æ—Å—Ç–æ–∏—Ç –º–∞–∫—Å–∏–º—É–º –∏–∑ 2 —Ä–∞–∑–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ - –≤–µ—Ä–æ—è—Ç–Ω–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
                continue
            # –£–¥–∞–ª—è–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            if len(word) > 1:
                filtered_words.append(word)
        
        text = " ".join(filtered_words)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences = text.split('. ')
        sentences = [s.capitalize() if s and len(s) > 1 else s for s in sentences]
        text = '. '.join(sentences)
        
        # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ —Ç–æ—á–∫–∏ –∏ –ø—Ä–æ–±–µ–ª—ã
        text = text.replace('..', '.')
        text = text.replace('  ', ' ')
        text = text.replace(' ,', ',')
        text = text.replace(' .', '.')
        text = text.replace('..', '.')
        
        # –£–¥–∞–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ —Å–æ—Å—Ç–æ—è—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        if len(text.strip()) < 3:
            return ""
        
        return text.strip()
    
    def format_output(self, 
                     transcription: Dict,
                     pauses: List[Tuple[float, float]],
                     include_speakers: bool = True) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç.
        """
        output_lines = []
        
        segments = transcription.get('segments', [])
        if not segments:
            return transcription.get('text', '')
        
        current_time = 0.0
        
        for i, segment in enumerate(segments):
            segment_start = segment['start']
            segment_end = segment['end']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—É–∑–µ –ø–µ—Ä–µ–¥ —Å–µ–≥–º–µ–Ω—Ç–æ–º, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            for pause_start, pause_end in pauses:
                if pause_start < segment_start < pause_end:
                    pause_duration = pause_end - pause_start
                    output_lines.append(f"\n[–ü–ê–£–ó–ê: {pause_duration:.1f} —Å–µ–∫]\n")
                    break
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–∏–∫–µ—Ä–µ
            speaker_info = ""
            if include_speakers and 'speaker' in segment:
                speaker_info = f"[{segment['speaker']}] "
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è
            time_info = f"[{self.format_time(segment_start)} - {self.format_time(segment_end)}]"
            
            # –¢–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞ —Å –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            text = self.post_process_text(segment.get('text', ''))
            
            output_lines.append(f"{time_info} {speaker_info}{text}")
        
        return "\n".join(output_lines)
    
    def format_time(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç."""
        td = timedelta(seconds=int(seconds))
        total_seconds = int(seconds)
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        secs = total_seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    def split_audio_into_chunks(self, audio_path: str, chunk_duration: float = 30.0) -> List[Tuple[str, float, float]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø—É—Ç—å_–∫_—á–∞–Ω–∫—É, –Ω–∞—á–∞–ª–æ, –∫–æ–Ω–µ—Ü)
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(audio_path):
            raise FileNotFoundError(
                f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}\n"
                f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ."
            )
        
        try:
            audio = AudioSegment.from_file(audio_path)
        except Exception as e:
            raise FileNotFoundError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª: {audio_path}\n"
                f"–û—à–∏–±–∫–∞: {e}\n"
                f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç."
            )
        duration_seconds = len(audio) / 1000.0
        
        chunks = []
        temp_dir = Path(audio_path).parent / "temp_chunks"
        temp_dir.mkdir(exist_ok=True)
        
        chunk_num = 0
        start_time = 0.0
        
        while start_time < duration_seconds:
            end_time = min(start_time + chunk_duration, duration_seconds)
            
            chunk_audio = audio[int(start_time * 1000):int(end_time * 1000)]
            chunk_path = temp_dir / f"chunk_{chunk_num:04d}.wav"
            chunk_audio.export(str(chunk_path), format="wav")
            
            chunks.append((str(chunk_path), start_time, end_time))
            start_time = end_time
            chunk_num += 1
        
        return chunks
    
    def write_segment_to_file(self, segment_text: str, append: bool = True):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ —Ñ–∞–π–ª."""
        # –ï—Å–ª–∏ output_file –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–≤–µ–±-—Ä–µ–∂–∏–º), –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        if self.output_file:
            with self.lock:
                try:
                    mode = 'a' if append else 'w'
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º errors='replace' –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    with open(self.output_file, mode, encoding='utf-8', errors='replace') as f:
                        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–µ–∫—Å—Ç - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω
                        if isinstance(segment_text, bytes):
                            segment_text = segment_text.decode('utf-8', errors='replace')
                        f.write(segment_text)
                        f.flush()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞ –¥–∏—Å–∫
                        if os.name != 'nt':  # –î–ª—è Unix-—Å–∏—Å—Ç–µ–º
                            os.fsync(f.fileno())  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å –¥–∏—Å–∫–æ–º
                except UnicodeEncodeError as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª: {e}")
                    # –ü—Ä–æ–±—É–µ–º –∑–∞–ø–∏—Å–∞—Ç—å —Å –∑–∞–º–µ–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    try:
                        with open(self.output_file, mode, encoding='utf-8', errors='replace') as f:
                            safe_text = segment_text.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
                            f.write(safe_text)
                            f.flush()
                    except Exception as e2:
                        print(f"‚ö†Ô∏è  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e2}")
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª {self.output_file}: {e}")
    
    def process_chunk_streaming(self, 
                               chunk_path: str,
                               global_offset: float,
                               language: str = "ru",
                               last_segment_end: Optional[float] = None) -> Tuple[List[Dict], float, float]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ –∞—É–¥–∏–æ –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Å–µ–≥–º–µ–Ω—Ç—ã, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å_–ø–∞—É–∑—ã, –ø–æ—Å–ª–µ–¥–Ω–µ–µ_–≤—Ä–µ–º—è_–∫–æ–Ω—Ü–∞)
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ soundfile (–æ–±—Ö–æ–¥ FFmpeg)
        try:
            if AUDIO_AVAILABLE:
                audio_data, sample_rate = sf.read(chunk_path)
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ–Ω–æ, –µ—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ
                if len(audio_data.shape) > 1:
                    audio_data = np.mean(audio_data, axis=1)
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [-1, 1] –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if audio_data.dtype != np.float32:
                    if audio_data.dtype == np.int16:
                        audio_data = audio_data.astype(np.float32) / 32768.0
                    elif audio_data.dtype == np.int32:
                        audio_data = audio_data.astype(np.float32) / 2147483648.0
                    else:
                        audio_data = audio_data.astype(np.float32)
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —ç—Ç–æ numpy –º–∞—Å—Å–∏–≤ (–Ω–µ torch tensor)
                if hasattr(audio_data, 'cpu'):
                    audio_data = audio_data.cpu().numpy()
                elif hasattr(audio_data, 'numpy'):
                    audio_data = audio_data.numpy()
                audio_data = np.asarray(audio_data, dtype=np.float32)
                
                # MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç word_timestamps (—Ç—Ä–µ–±—É–µ—Ç float64)
                use_word_timestamps = self.device != "mps"
                if not use_word_timestamps and self.device == "mps":
                    print("  (word_timestamps –æ—Ç–∫–ª—é—á–µ–Ω—ã –¥–ª—è MPS)")
                
                # –ü–µ—Ä–µ–¥–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é numpy –º–∞—Å—Å–∏–≤ –≤–º–µ—Å—Ç–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
                result = self.whisper_model.transcribe(
                    audio_data,
                    language=language,
                    word_timestamps=use_word_timestamps,
                    verbose=False,
                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    beam_size=5,  # Beam search –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
                    condition_on_previous_text=True,  # –£—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    temperature=0.0,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    best_of=5,  # –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                    compression_ratio_threshold=2.2,  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π (–±—ã–ª–æ 2.4)
                    logprob_threshold=-0.5,  # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–±—ã–ª–æ -1.0)
                    no_speech_threshold=0.4  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ (–±—ã–ª–æ 0.6)
                )
            else:
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥ (—Ç—Ä–µ–±—É–µ—Ç FFmpeg)
                # MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç word_timestamps
                use_word_timestamps = self.device != "mps"
                result = self.whisper_model.transcribe(
                    chunk_path,
                    language=language,
                    word_timestamps=use_word_timestamps,
                    verbose=False,
                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    beam_size=5,
                    condition_on_previous_text=True,
                    temperature=0.0,
                    best_of=5,
                    compression_ratio_threshold=2.4,
                    logprob_threshold=-1.0,
                    no_speech_threshold=0.6
                )
        except Exception as e:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ soundfile, –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥
            print(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ soundfile, –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {e}")
            # MPS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç word_timestamps
            use_word_timestamps = self.device != "mps"
            result = self.whisper_model.transcribe(
                chunk_path,
                language=language,
                word_timestamps=use_word_timestamps,
                verbose=False,
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                beam_size=5,
                condition_on_previous_text=True,
                temperature=0.0,
                best_of=5,
                compression_ratio_threshold=2.4,
                logprob_threshold=-1.0,
                no_speech_threshold=0.6
            )
        
        segments = result.get('segments', [])
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ —Å —É—á–µ—Ç–æ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è
        for segment in segments:
            segment['start'] += global_offset
            segment['end'] += global_offset
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—É–∑—É –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        pause_duration = 0.0
        if last_segment_end is not None and segments:
            pause_start = last_segment_end
            pause_end = segments[0]['start']
            pause_duration = pause_end - pause_start
        
        return segments, pause_duration, segments[-1]['end'] if segments else global_offset
    
    def process_lecture_streaming(self,
                                 audio_path: str,
                                 language: str = "ru",
                                 output_path: str = None,
                                 auth_token: str = None,
                                 include_speakers: bool = False,
                                 chunk_duration: float = 30.0) -> str:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª–µ–∫—Ü–∏—é –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ, –∑–∞–ø–∏—Å—ã–≤–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª –ø–æ –º–µ—Ä–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            auth_token: –¢–æ–∫–µ–Ω Hugging Face –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            include_speakers: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
            chunk_duration: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        
        Returns:
            –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(audio_path):
            raise FileNotFoundError(
                f"\n‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}\n"
                f"  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ.\n"
                f"  –ü—Ä–∏–º–µ—Ä: python3 transcribe_lecture.py –≤–∞—à_—Ñ–∞–π–ª.mp3 --streaming"
            )
        
        if output_path:
            self.output_file = output_path
            # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
            with open(output_path, 'w', encoding='utf-8', errors='replace') as f:
                f.write("=" * 60 + "\n")
                f.write("–¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –õ–ï–ö–¶–ò–ò (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏)\n")
                f.write(f"–§–∞–π–ª: {Path(audio_path).name}\n")
                f.write(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")
                f.flush()
        
        print(f"–ù–∞—á–∞–ª–æ –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {audio_path}")
        print(f"–Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {language}")
        print(f"–ß–∞–Ω–∫–∏ –ø–æ {chunk_duration} —Å–µ–∫—É–Ω–¥")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –∞—É–¥–∏–æ –Ω–∞ —á–∞–Ω–∫–∏
        print("–†–∞–∑–±–∏–≤–∫–∞ –∞—É–¥–∏–æ –Ω–∞ —á–∞–Ω–∫–∏...")
        chunks = self.split_audio_into_chunks(audio_path, chunk_duration)
        total_chunks = len(chunks)
        print(f"–°–æ–∑–¥–∞–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n")
        
        all_segments = []
        last_segment_end = None
        temp_chunks_dir = Path(audio_path).parent / "temp_chunks"
        
        try:
            for i, (chunk_path, start_time, end_time) in enumerate(chunks, 1):
                print(f"[{i}/{total_chunks}] –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ ({self.format_time(start_time)} - {self.format_time(end_time)})...", end=' ', flush=True)
                
                start_process_time = time.time()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫
                segments, pause_duration, last_end = self.process_chunk_streaming(
                    chunk_path,
                    start_time,
                    language,
                    last_segment_end
                )
                
                process_time = time.time() - start_process_time
                print(f"‚úì ({process_time:.1f}—Å)")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—É–∑—É –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º —Å–µ–≥–º–µ–Ω—Ç–æ–º —á–∞–Ω–∫–∞
                if pause_duration >= self.min_pause_duration and i > 1:
                    pause_text = f"\n[–ü–ê–£–ó–ê: {pause_duration:.1f} —Å–µ–∫]\n\n"
                    if output_path:
                        self.write_segment_to_file(pause_text)
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –≤ —Ñ–∞–π–ª –ø–æ –º–µ—Ä–µ –ø–æ–ª—É—á–µ–Ω–∏—è
                for segment in segments:
                    segment_start = segment['start']
                    segment_end = segment['end']
                    text = segment.get('text', '').strip()
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç
                    speaker_info = ""
                    if include_speakers and 'speaker' in segment:
                        speaker_info = f"[{segment['speaker']}] "
                    
                    time_info = f"[{self.format_time(segment_start)} - {self.format_time(segment_end)}]"
                    segment_text = f"{time_info} {speaker_info}{text}\n"
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
                    if output_path:
                        self.write_segment_to_file(segment_text)
                    
                    all_segments.append(segment)
                
                last_segment_end = last_end
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress = (i / total_chunks) * 100
                print(f"  –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% | –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(all_segments)}")
                
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —á–∞–Ω–∫ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
                try:
                    Path(chunk_path).unlink()
                except:
                    pass
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            try:
                temp_chunks_dir.rmdir()
            except:
                pass
            
            print(f"\n‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(all_segments)}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ñ–∞–π–ª
            if output_path:
                summary_text = f"\n\n{'='*60}\n"
                summary_text += f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                summary_text += f"–í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(all_segments)}\n"
                summary_text += f"{'='*60}\n"
                self.write_segment_to_file(summary_text)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            if output_path:
                json_path = output_path.replace('.txt', '.json')
                with open(json_path, 'w', encoding='utf-8', errors='replace') as f:
                    json.dump({
                        'segments': all_segments,
                        'total_segments': len(all_segments),
                        'audio_path': audio_path,
                        'processed_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'chunk_duration': chunk_duration
                    }, f, ensure_ascii=False, indent=2)
                print(f"–ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {json_path}")
            
            return output_path
            
        except Exception as e:
            print(f"\n‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –æ—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            try:
                import shutil
                if temp_chunks_dir.exists():
                    shutil.rmtree(temp_chunks_dir)
            except:
                pass
            raise
    
    def record_and_transcribe_live(self,
                                   output_path: str,
                                   language: str = "ru",
                                   audio_device: Optional[int] = None,
                                   system_audio: bool = False,
                                   chunk_duration: float = 30.0,
                                   save_audio: bool = True,
                                   text_callback: Optional[Callable[[str], None]] = None) -> str:
        """
        –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∞—É–¥–∏–æ —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞/—Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.
        
        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ
            audio_device: ID –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (None = –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            system_audio: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ –≤–º–µ—Å—Ç–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
            chunk_duration: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            save_audio: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –∞—É–¥–∏–æ —Ñ–∞–π–ª
        
        Returns:
            –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
        """
        if not AUDIO_AVAILABLE:
            raise ImportError("sounddevice –∏ soundfile –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ")
        
        # –ï—Å–ª–∏ output_path —ç—Ç–æ /dev/null –∏–ª–∏ nul, –Ω–µ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª
        self.output_file = output_path if output_path not in ["/dev/null", "nul"] else None
        self.is_live_recording = True
        self.text_callback = text_callback  # Callback –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –≤ GUI
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å
        if self.output_file:
            with open(output_path, 'w', encoding='utf-8', errors='replace') as f:
                f.write("=" * 60 + "\n")
                f.write("–¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø –õ–ï–ö–¶–ò–ò (–∑–∞–ø–∏—Å—å –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏)\n")
                f.write(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {'–°–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ (–¥–∏–Ω–∞–º–∏–∫–∏)' if system_audio else '–ú–∏–∫—Ä–æ—Ñ–æ–Ω'}\n")
                f.write(f"–ù–∞—á–∞–ª–æ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")
                f.flush()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–ø–∏—Å–∏
        if system_audio:
            recorder = AudioRecorder()
            device_id = recorder.find_system_audio_device()
            if device_id is None:
                print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∑–≤—É–∫–∞.")
                print("   –î–ª—è macOS —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å BlackHole:")
                print("   https://github.com/ExistentialAudio/BlackHole")
                print("   –ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")
                device_id = None
        else:
            device_id = audio_device
        
        # –î–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏ (–±—ã—Å—Ç—Ä–µ–µ, –∫–∞–∫ –Ω–∞ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ)
        if self.use_system_recognizer:
            chunk_duration = 5.0  # 5 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–ø–∏—Å–∏
        else:
            chunk_duration = chunk_duration  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è Whisper
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —á–∞–Ω–∫–æ–≤
        # –ï—Å–ª–∏ output_path —ç—Ç–æ /dev/null –∏–ª–∏ nul, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        if output_path in ["/dev/null", "nul"]:
            import tempfile
            temp_base = Path(tempfile.gettempdir())
            temp_dir = temp_base / "lecture_transcribe_chunks"
        else:
            temp_dir = Path(output_path).parent / "temp_live_chunks"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
        try:
            temp_dir.mkdir(exist_ok=True, parents=True, mode=0o755)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–∂–µ–º –ø–∏—Å–∞—Ç—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            test_file = temp_dir / ".test_write"
            test_file.touch()
            test_file.unlink()
        except (PermissionError, OSError) as e:
            # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Å–æ–∑–¥–∞—Ç—å –≤ temp, –ø—Ä–æ–±—É–µ–º –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            temp_dir = Path.cwd() / "temp_live_chunks"
            try:
                temp_dir.mkdir(exist_ok=True, parents=True, mode=0o755)
            except Exception:
                raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {e}")
        
        print(f"\n{'='*60}")
        print(f"{'üé§ –ó–ê–ü–ò–°–¨ –° –ú–ò–ö–†–û–§–û–ù–ê' if not system_audio else 'üîä –ó–ê–ü–ò–°–¨ –°–ò–°–¢–ï–ú–ù–û–ì–û –ó–í–£–ö–ê'}")
        print(f"{'='*60}")
        print(f"–Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {language}")
        print(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤: {output_path}")
        if self.use_system_recognizer:
            print(f"–†–µ–∂–∏–º: –°–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å (–∫–∞–∫ –Ω–∞ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞)")
        print(f"–ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n")
        
        try:
            if audio_device is None and not system_audio and not self.use_system_recognizer:
                print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –∑–∞–ø–∏—Å–∏")
            
            recording_start_time = time.time()
            chunk_counter = 0
            all_segments = []
            last_segment_end = None
            last_words = []  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            
            # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (—Ç–æ–ª—å–∫–æ –≤ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ)
            def signal_handler(sig, frame):
                print("\n\n‚èπÔ∏è  –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏...")
                self.is_live_recording = False
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ Ctrl+C
            # –í –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–µ —ç—Ç–æ –±—É–¥–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            # ValueError –≤–æ–∑–Ω–∏–∫–∞–µ—Ç, –µ—Å–ª–∏ –º—ã –ø—ã—Ç–∞–µ–º—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å signal –Ω–µ –≤ –≥–ª–∞–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            try:
                signal.signal(signal.SIGINT, signal_handler)
            except ValueError:
                # ValueError: signal only works in main thread
                # –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ - –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ WebSocket –∏ —Ñ–ª–∞–≥ is_live_recording
                pass
            except Exception:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫
                # –í –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–µ signal –Ω–µ –Ω—É–∂–µ–Ω, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ WebSocket
                pass
            
            # –ü–æ—Ç–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ
            audio_queue = queue.Queue()
            
            def audio_callback(indata, frames, time_info, status):
                if self.is_live_recording:
                    audio_queue.put(indata.copy())
            
            if not self.use_system_recognizer:
                print("üé§ –ù–∞—á–∞–ª–æ –∑–∞–ø–∏—Å–∏...\n")
            else:
                print()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è
            
            # –î–ª—è —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–π blocksize
            blocksize_duration = 0.1 if (self.use_system_recognizer and chunk_duration <= 0.3) else 0.5
            with sd.InputStream(samplerate=16000,
                              channels=1,
                              device=device_id,
                              dtype='float32',
                              callback=audio_callback,
                              blocksize=int(16000 * blocksize_duration)):  # –ë–ª–æ–∫–∏ –ø–æ 0.1-0.5 —Å–µ–∫—É–Ω–¥—ã
                
                chunk_data = []
                overlap_data = []  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
                chunk_start_time = time.time()
                # –î–ª—è —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
                if self.use_system_recognizer and chunk_duration <= 0.3:
                    overlap_duration = 0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞
                else:
                    overlap_duration = 1.5  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ 1.5 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ—Ç–µ—Ä–∏ —Å–ª–æ–≤
                overlap_samples = int(16000 * overlap_duration)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                
                # –†–µ–∂–∏–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è (chunk_duration <= 1.0)
                realtime_mode = self.use_system_recognizer and chunk_duration <= 1.0
                if realtime_mode:
                    # –î–ª—è —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞ (<= 0.3 —Å–µ–∫) –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π –±—É—Ñ–µ—Ä
                    # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (0.3-1.0 —Å–µ–∫) –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—É—Ñ–µ—Ä –ø–æ–±–æ–ª—å—à–µ
                    if chunk_duration <= 0.3:
                        # –£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–µ 0.2-0.3 —Å–µ–∫—É–Ω–¥—ã
                        realtime_buffer_duration = max(0.2, chunk_duration)
                    else:
                        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: –±—É—Ñ–µ—Ä 0.5-1 —Å–µ–∫—É–Ω–¥–∞
                        realtime_buffer_duration = min(1.0, chunk_duration * 1.5)
                    realtime_buffer_samples = int(16000 * realtime_buffer_duration)
                    realtime_buffer = []
                    realtime_last_process = time.time()
                
                while self.is_live_recording:
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                        try:
                            audio_block = audio_queue.get(timeout=0.5)
                        except queue.Empty:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ
                            if not self.is_live_recording:
                                break
                            continue
                        
                        current_time = time.time()
                        
                        # –†–µ–∂–∏–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è
                        if realtime_mode:
                            realtime_buffer.append(audio_block)
                            buffer_samples = sum(len(block) for block in realtime_buffer)
                            
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É –∏–ª–∏ –∫–æ–≥–¥–∞ –±—É—Ñ–µ—Ä –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π
                            elapsed_since_process = current_time - realtime_last_process
                            if elapsed_since_process >= realtime_buffer_duration or buffer_samples >= realtime_buffer_samples:
                                if len(realtime_buffer) > 0:
                                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±—É—Ñ–µ—Ä
                                    buffer_array = np.concatenate(realtime_buffer, axis=0)
                                    if len(buffer_array.shape) == 1:
                                        buffer_array = buffer_array.reshape(-1, 1)
                                    
                                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                                    if len(overlap_data) > 0:
                                        overlap_array = np.concatenate(overlap_data, axis=0)
                                        buffer_array = np.concatenate([overlap_array, buffer_array], axis=0)
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                                    total_samples = len(buffer_array)
                                    if total_samples > overlap_samples:
                                        overlap_start_idx = total_samples - overlap_samples
                                        overlap_data = [buffer_array[overlap_start_idx:]]
                                    else:
                                        overlap_data = [buffer_array]
                                    
                                    # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Å—Ä–∞–∑—É –∏–∑ –±—É—Ñ–µ—Ä–∞
                                    system_lang = get_language_code(language)
                                    text = self.system_recognizer.recognize_audio_data(buffer_array, sample_rate=16000, language=system_lang)
                                    
                                    if text:
                                        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                                        words = text.split()
                                        if len(last_words) > 0 and len(words) > 0:
                                            check_len = min(5, len(last_words), len(words))
                                            if check_len > 0:
                                                last_words_check = last_words[-check_len:]
                                                first_words_check = words[:check_len]
                                                
                                                if last_words_check == first_words_check:
                                                    words = words[check_len:]
                                                elif check_len >= 3:
                                                    for i in range(2, check_len + 1):
                                                        if last_words[-i:] == words[:i]:
                                                            words = words[i:]
                                                            break
                                        
                                        if len(words) > 0:
                                            last_words = words[-5:]
                                            text = ' '.join(words)
                                            
                                            if text.strip():
                                                self.write_segment_to_file(f"{text}\n")
                                                if self.text_callback:
                                                    self.text_callback(text)
                                    
                                    # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä
                                    realtime_buffer = []
                                    realtime_last_process = current_time
                            
                            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—ã—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —á–∞–Ω–∫–æ–≤
                        
                        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º (–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤)
                        chunk_data.append(audio_block)
                        elapsed = current_time - chunk_start_time
                        
                        # –ö–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞–Ω–∫–∞
                        if elapsed >= chunk_duration:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫
                            chunk_path = temp_dir / f"chunk_{chunk_counter:04d}.wav"
                            try:
                                chunk_array = np.concatenate(chunk_data, axis=0)
                            except Exception as e:
                                print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö: {e}")
                                chunk_data = []
                                chunk_start_time = current_time
                                continue
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —á–∞–Ω–∫–æ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
                            if len(overlap_data) > 0:
                                overlap_array = np.concatenate(overlap_data, axis=0)
                                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Å —Ç–µ–∫—É—â–∏–º —á–∞–Ω–∫–æ–º
                                chunk_array = np.concatenate([overlap_array, chunk_array], axis=0)
                            
                            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–∞—Å—Å–∏–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω
                            if len(chunk_array.shape) == 1:
                                # –ï—Å–ª–∏ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤, –¥–æ–±–∞–≤–ª—è–µ–º –æ—Å—å –¥–ª—è –∫–∞–Ω–∞–ª–æ–≤
                                chunk_array = chunk_array.reshape(-1, 1)
                            
                            try:
                                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏
                                if not temp_dir.exists():
                                    temp_dir.mkdir(exist_ok=True, parents=True, mode=0o755)
                                sf.write(str(chunk_path), chunk_array, 16000, subtype='PCM_16')
                            except (PermissionError, OSError) as e:
                                print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —á–∞–Ω–∫–∞ {chunk_counter}: {e}")
                                # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                                try:
                                    import tempfile
                                    alt_temp_dir = Path(tempfile.gettempdir()) / f"lecture_chunks_{os.getpid()}"
                                    alt_temp_dir.mkdir(exist_ok=True, parents=True, mode=0o755)
                                    temp_dir = alt_temp_dir
                                    chunk_path = temp_dir / f"chunk_{chunk_counter:04d}.wav"
                                    sf.write(str(chunk_path), chunk_array, 16000, subtype='PCM_16')
                                except Exception as e2:
                                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –≤ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {e2}")
                                    chunk_data = []
                                    chunk_start_time = current_time
                                    continue
                            except Exception as e:
                                print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —á–∞–Ω–∫–∞ {chunk_counter}: {e}")
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —á–∞–Ω–∫ –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                                chunk_data = []
                                chunk_start_time = current_time
                                continue
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
                            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ overlap_samples —Å—ç–º–ø–ª–æ–≤
                            total_samples = len(chunk_array)
                            if total_samples > overlap_samples:
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ overlap_samples –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
                                overlap_start_idx = total_samples - overlap_samples
                                overlap_data = [chunk_array[overlap_start_idx:]]
                            else:
                                # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å—å
                                overlap_data = [chunk_array]
                            
                            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —á–∞–Ω–∫
                            global_offset = chunk_counter * chunk_duration
                            
                            if self.use_system_recognizer:
                                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å (–∫–∞–∫ –Ω–∞ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞)
                                # –ë–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤, —Å—Ä–∞–∑—É –ø–∏—à–µ–º –≤ —Ñ–∞–π–ª
                                system_lang = get_language_code(language)
                                text = self.system_recognizer.recognize_audio_file(str(chunk_path), language=system_lang)
                                
                                if text:
                                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–µ–π—Å—è —á–∞—Å—Ç–∏
                                    words = text.split()
                                    if len(last_words) > 0 and len(words) > 0:
                                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
                                        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3-5 —Å–ª–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞ —Å –ø–µ—Ä–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –Ω–æ–≤–æ–≥–æ
                                        check_len = min(5, len(last_words), len(words))
                                        if check_len > 0:
                                            last_words_check = last_words[-check_len:]
                                            first_words_check = words[:check_len]
                                            
                                            # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç, —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                                            if last_words_check == first_words_check:
                                                words = words[check_len:]
                                            # –ï—Å–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞—é—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –∏–∑ 3)
                                            elif check_len >= 3:
                                                for i in range(2, check_len + 1):
                                                    if last_words[-i:] == words[:i]:
                                                        words = words[i:]
                                                        break
                                    
                                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
                                    if len(words) > 0:
                                        last_words = words[-5:]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–ª–æ–≤
                                        text = ' '.join(words)
                                        
                                        # –°—Ä–∞–∑—É –ø–∏—à–µ–º –≤ —Ñ–∞–π–ª –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ (–∫–∞–∫ –Ω–∞ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ)
                                        if text.strip():  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                                            self.write_segment_to_file(f"{text}\n")
                                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ GUI —á–µ—Ä–µ–∑ callback
                                            if self.text_callback:
                                                self.text_callback(text)
                                            all_segments.append({
                                                'start': global_offset,
                                                'end': global_offset + chunk_duration,
                                                'text': text
                                            })
                                
                                pause_duration = 0.0
                                last_end = global_offset + chunk_duration if text else last_segment_end
                                
                                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç–æ–π —Å—Ç–∞—Ç—É—Å
                                total_time = time.time() - recording_start_time
                                print(f"‚úì {self.format_time(total_time)} | –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(all_segments)}")
                            else:
                                # –ò—Å–ø–æ–ª—å–∑—É–µ–º Whisper (—Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏)
                                print(f"[–ß–∞–Ω–∫ {chunk_counter + 1}] –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è ({self.format_time(global_offset)})...", end=' ', flush=True)
                                
                                start_transcribe = time.time()
                                
                                segments, pause_duration, last_end = self.process_chunk_streaming(
                                    str(chunk_path),
                                    global_offset,
                                    language,
                                    last_segment_end
                                )
                                
                                transcribe_time = time.time() - start_transcribe
                                print(f"‚úì ({transcribe_time:.1f}—Å)")
                                
                                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—É–∑—É
                                if pause_duration >= self.min_pause_duration and chunk_counter > 0:
                                    pause_text = f"\n[–ü–ê–£–ó–ê: {pause_duration:.1f} —Å–µ–∫]\n\n"
                                    self.write_segment_to_file(pause_text)
                                
                                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –≤ —Ñ–∞–π–ª
                                for segment in segments:
                                    segment_start = segment['start']
                                    segment_end = segment['end']
                                    text = self.post_process_text(segment.get('text', ''))
                                    
                                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                                    if not text or len(text.strip()) < 3:
                                        continue
                                    
                                    time_info = f"[{self.format_time(segment_start)} - {self.format_time(segment_end)}]"
                                    segment_text = f"{time_info} {text}\n"
                                    
                                    self.write_segment_to_file(segment_text)
                                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ GUI —á–µ—Ä–µ–∑ callback (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫)
                                    if self.text_callback:
                                        self.text_callback(text)
                                    all_segments.append(segment)
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
                                    if text:
                                        self.previous_text = text[-100:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                            
                            last_segment_end = last_end
                            chunk_counter += 1
                            
                            # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
                            chunk_data = []
                            chunk_start_time = current_time
                            
                            # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —á–∞–Ω–∫
                            try:
                                chunk_path.unlink()
                            except:
                                pass
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å (—Ç–æ–ª—å–∫–æ –¥–ª—è Whisper)
                            if not self.use_system_recognizer:
                                total_time = time.time() - recording_start_time
                                print(f"  –í—Ä–µ–º—è –∑–∞–ø–∏—Å–∏: {self.format_time(total_time)} | –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(all_segments)}")
                    
                    except queue.Empty:
                        continue
                    except KeyboardInterrupt:
                        break
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ
            if chunk_data:
                chunk_path = temp_dir / f"chunk_{chunk_counter:04d}.wav"
                chunk_array = np.concatenate(chunk_data, axis=0)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —á–∞–Ω–∫–æ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if len(overlap_data) > 0:
                    overlap_array = np.concatenate(overlap_data, axis=0)
                    chunk_array = np.concatenate([overlap_array, chunk_array], axis=0)
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–∞—Å—Å–∏–≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω
                if len(chunk_array.shape) == 1:
                    chunk_array = chunk_array.reshape(-1, 1)
                
                try:
                    sf.write(str(chunk_path), chunk_array, 16000, subtype='PCM_16')
                except Exception as e:
                    print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —á–∞–Ω–∫–∞: {e}")
                    # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫
                    return output_path
                
                global_offset = chunk_counter * chunk_duration
                
                if self.use_system_recognizer:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å (–±–µ–∑ —Å–æ–æ–±—â–µ–Ω–∏–π, —Å—Ä–∞–∑—É –ø–∏—à–µ–º)
                    system_lang = get_language_code(language)
                    text = self.system_recognizer.recognize_audio_file(str(chunk_path), language=system_lang)
                    
                    if text:
                        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–µ–π—Å—è —á–∞—Å—Ç–∏
                        words = text.split()
                        if len(last_words) > 0 and len(words) > 0:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
                            check_len = min(5, len(last_words), len(words))
                            if check_len > 0:
                                last_words_check = last_words[-check_len:]
                                first_words_check = words[:check_len]
                                
                                # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç, —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                                if last_words_check == first_words_check:
                                    words = words[check_len:]
                                # –ï—Å–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞—é—Ç
                                elif check_len >= 3:
                                    for i in range(2, check_len + 1):
                                        if last_words[-i:] == words[:i]:
                                            words = words[i:]
                                            break
                        
                        if len(words) > 0:
                            text = ' '.join(words)
                            # –°—Ä–∞–∑—É –ø–∏—à–µ–º –≤ —Ñ–∞–π–ª –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ (–∫–∞–∫ –Ω–∞ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ)
                            if text.strip():
                                self.write_segment_to_file(f"{text}\n")
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ GUI —á–µ—Ä–µ–∑ callback
                                if self.text_callback:
                                    self.text_callback(text)
                                all_segments.append({
                                    'start': global_offset,
                                    'end': global_offset + len(chunk_array) / 16000,
                                    'text': text
                                })
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Whisper (—Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏)
                    print(f"\n[–§–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫] –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è...", end=' ', flush=True)
                    
                    segments, _, _ = self.process_chunk_streaming(
                        str(chunk_path),
                        global_offset,
                        language,
                        last_segment_end
                    )
                    
                    for segment in segments:
                        segment_start = segment['start']
                        segment_end = segment['end']
                        text = self.post_process_text(segment.get('text', ''))
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                        if not text or len(text.strip()) < 3:
                            continue
                        
                        time_info = f"[{self.format_time(segment_start)} - {self.format_time(segment_end)}]"
                        segment_text = f"{time_info} {text}\n"
                        
                        self.write_segment_to_file(segment_text)
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ GUI —á–µ—Ä–µ–∑ callback (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫)
                        if self.text_callback:
                            self.text_callback(text)
                        all_segments.append(segment)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
                        if text:
                            self.previous_text = text[-100:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                    
                    print("‚úì")
                
                try:
                    chunk_path.unlink()
                except:
                    pass
            
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            try:
                temp_dir.rmdir()
            except:
                pass
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            total_time = time.time() - recording_start_time
            summary_text = f"\n\n{'='*60}\n"
            summary_text += f"–ó–∞–ø–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
            summary_text += f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {self.format_time(total_time)}\n"
            summary_text += f"–í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(all_segments)}\n"
            summary_text += f"{'='*60}\n"
            self.write_segment_to_file(summary_text)
            
            print(f"\n{'='*60}")
            print(f"‚úì –ó–∞–ø–∏—Å—å –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
            print(f"  –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(all_segments)}")
            print(f"  –í—Ä–µ–º—è –∑–∞–ø–∏—Å–∏: {self.format_time(total_time)}")
            print(f"  –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_path}")
            print(f"{'='*60}\n")
            
            return output_path
            
        except Exception as e:
            print(f"\n‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏: {e}")
            import traceback
            traceback.print_exc()
            self.is_live_recording = False
            raise
    
    def process_lecture(self, 
                       audio_path: str,
                       language: str = "ru",
                       output_path: str = None,
                       auth_token: str = None,
                       include_speakers: bool = True) -> str:
        """
        –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–µ–∫—Ü–∏–∏.
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            auth_token: –¢–æ–∫–µ–Ω Hugging Face –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
            include_speakers: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
        
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(audio_path):
            raise FileNotFoundError(
                f"\n‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_path}\n"
                f"  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ.\n"
                f"  –ü—Ä–∏–º–µ—Ä: python3 transcribe_lecture.py –≤–∞—à_—Ñ–∞–π–ª.mp3"
            )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º pipeline –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if include_speakers and not self.diarization_pipeline:
            self.load_diarization_pipeline(auth_token)
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
        transcription = self.transcribe_audio(audio_path, language)
        
        # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤
        speaker_segments = []
        if include_speakers and self.diarization_pipeline:
            speaker_segments = self.diarize_speakers(audio_path)
            if speaker_segments:
                transcription['segments'] = self.assign_speakers_to_segments(
                    transcription['segments'],
                    speaker_segments
                )
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—É–∑
        print("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—É–∑...")
        pauses = self.detect_pauses(audio_path)
        print(f"–ù–∞–π–¥–µ–Ω–æ –ø–∞—É–∑: {len(pauses)}")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        output_text = self.format_output(transcription, pauses, include_speakers)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if output_path:
            with open(output_path, 'w', encoding='utf-8', errors='replace') as f:
                f.write(output_text)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
            
            # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            json_path = output_path.replace('.txt', '.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'transcription': transcription,
                    'pauses': pauses,
                    'speakers': speaker_segments
                }, f, ensure_ascii=False, indent=2)
            print(f"–ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {json_path}")
        
        return output_text


def main():
    parser = argparse.ArgumentParser(
        description="–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ –ª–µ–∫—Ü–∏–π —Å —É—á–µ—Ç–æ–º –ø–∞—É–∑ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ø–∏–∫–µ—Ä–æ–≤"
    )
    parser.add_argument(
        "audio_file",
        type=str,
        nargs='?',
        default=None,
        help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ --record)"
    )
    parser.add_argument(
        "-o", "--output",
        type=str,
        default=None,
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: <–∏–º—è_—Ñ–∞–π–ª–∞>_transcript.txt)"
    )
    parser.add_argument(
        "-l", "--language",
        type=str,
        default="ru",
        help="–Ø–∑—ã–∫ –∞—É–¥–∏–æ (ru, en, –∏ —Ç.–¥., –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ru)"
    )
    parser.add_argument(
        "-m", "--model",
        type=str,
        default="base",
        choices=["tiny", "base", "small", "medium", "large"],
        help="–ú–æ–¥–µ–ª—å Whisper (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: base)"
    )
    parser.add_argument(
        "--min-pause",
        type=float,
        default=1.0,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—É–∑—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1.0)"
    )
    parser.add_argument(
        "--token",
        type=str,
        default=None,
        help="Hugging Face —Ç–æ–∫–µ–Ω –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–ø–∏–∫–µ—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
    )
    parser.add_argument(
        "--no-speakers",
        action="store_true",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é —Å–ø–∏–∫–µ—Ä–æ–≤"
    )
    parser.add_argument(
        "--streaming",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ (–∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª –ø–æ –º–µ—Ä–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏)"
    )
    parser.add_argument(
        "--chunk-duration",
        type=float,
        default=30.0,
        help="–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 30.0)"
    )
    parser.add_argument(
        "--record",
        action="store_true",
        help="–ó–∞–ø–∏—Å—å —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞/–¥–∏–Ω–∞–º–∏–∫–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π"
    )
    parser.add_argument(
        "--system-audio",
        action="store_true",
        help="–ó–∞–ø–∏—Å—ã–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –∑–≤—É–∫ (–¥–∏–Ω–∞–º–∏–∫–∏) –≤–º–µ—Å—Ç–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞. –î–ª—è macOS —Ç—Ä–µ–±—É–µ—Ç—Å—è BlackHole."
    )
    parser.add_argument(
        "--audio-device",
        type=int,
        default=None,
        help="ID –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --list-devices –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞)"
    )
    parser.add_argument(
        "--list-devices",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—É–¥–∏–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –∏ –≤—ã–π—Ç–∏"
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        choices=["cpu", "cuda", "mps"],
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (cpu/cuda/mps, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è)"
    )
    parser.add_argument(
        "--use-system-recognizer",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏ (–∫–∞–∫ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ) –≤–º–µ—Å—Ç–æ Whisper"
    )
    parser.add_argument(
        "--recognizer-type",
        type=str,
        default="google",
        choices=["google", "sphinx"],
        help="–¢–∏–ø —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è (google - –æ–Ω–ª–∞–π–Ω, sphinx - –æ—Ñ–ª–∞–π–Ω, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: google)"
    )
    
    args = parser.parse_args()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ –≤—ã—Ö–æ–¥–∏–º
    if args.list_devices:
        if not AUDIO_AVAILABLE:
            print("‚ö†Ô∏è  sounddevice –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sounddevice soundfile")
            sys.exit(1)
        recorder = AudioRecorder()
        recorder.list_devices()
        sys.exit(0)
    
    # –†–µ–∂–∏–º –∑–∞–ø–∏—Å–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    if args.record:
        if not AUDIO_AVAILABLE:
            print("‚úó –û—à–∏–±–∫–∞: sounddevice –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
            print("  –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sounddevice soundfile")
            sys.exit(1)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –≤—ã–≤–æ–¥–∞
        if not args.output:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            source = "system" if args.system_audio else "mic"
            args.output = f"lecture_transcript_{source}_{timestamp}.txt"
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–µ—Ä
        transcriber = LectureTranscriber(
            whisper_model=args.model,
            min_pause_duration=args.min_pause,
            device=args.device,
            use_system_recognizer=args.use_system_recognizer,
            recognizer_type=args.recognizer_type
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å—å –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é
        result_path = transcriber.record_and_transcribe_live(
            output_path=args.output,
            language=args.language,
            audio_device=args.audio_device,
            system_audio=args.system_audio,
            chunk_duration=args.chunk_duration
        )
        
        print(f"\n‚úì –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {result_path}")
        sys.exit(0)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—ã—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if not args.audio_file:
        parser.error("audio_file –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è --record")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –≤—ã–≤–æ–¥–∞
    if not args.output:
        audio_path = Path(args.audio_file)
        args.output = str(audio_path.parent / f"{audio_path.stem}_transcript.txt")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–µ—Ä
    transcriber = LectureTranscriber(
        whisper_model=args.model,
        min_pause_duration=args.min_pause,
        device=args.device,
        use_system_recognizer=args.use_system_recognizer,
        recognizer_type=args.recognizer_type
    )
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if args.streaming:
        # –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        result_path = transcriber.process_lecture_streaming(
            audio_path=args.audio_file,
            language=args.language,
            output_path=args.output,
            auth_token=args.token,
            include_speakers=not args.no_speakers,
            chunk_duration=args.chunk_duration
        )
        print(f"\n‚úì –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {result_path}")
        print("\n–í—ã –º–æ–∂–µ—Ç–µ –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª –∏ —Å–ª–µ–¥–∏—Ç—å –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏!")
    else:
        # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–≤–µ—Å—å —Ñ–∞–π–ª —Å—Ä–∞–∑—É)
        result = transcriber.process_lecture(
            audio_path=args.audio_file,
            language=args.language,
            output_path=args.output,
            auth_token=args.token,
            include_speakers=not args.no_speakers
        )
        
        print("\n" + "="*50)
        print("–¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø:")
        print("="*50)
        print(result)


if __name__ == "__main__":
    main()

